{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering casa_cluster_austin_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook automatically generated from your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model KMeans (k=3), trained on 2019-04-09 22:17:11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated on 2019-04-20 18:17:39.942653"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering\n",
    "This notebook will reproduce the steps for clustering the dataset casa_cluster_austin_only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to provide an easily readable and explainable code that reproduces the main steps\n",
    "of training the model. It is not complete: some of the preprocessing done by the DSS visual machine learning is not\n",
    "replicated in this notebook. This notebook will not give the same results and model performance as the DSS visual machine\n",
    "learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with importing the required libs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import dataiku\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import dataiku.core.pandasutils as pdu\n",
    "from dataiku.doctor.preprocessing import PCA\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And tune pandas display options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 3000)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing base data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get our machine learning dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the preparation that you defined. You should not modify this.\n",
    "preparation_steps = []\n",
    "preparation_output_schema = {u'userModified': False, u'columns': [{u'type': u'string', u'name': u'@odata.context'}, {u'type': u'string', u'name': u'@odata.id'}, {u'type': u'bigint', u'name': u'BathroomsFull'}, {u'type': u'double', u'name': u'BathroomsHalf'}, {u'type': u'bigint', u'name': u'BedroomsTotal'}, {u'type': u'string', u'name': u'City'}, {u'type': u'string', u'name': u'Coordinates'}, {u'type': u'bigint', u'name': u'CoveredSpaces'}, {u'type': u'double', u'name': u'FireplacesTotal'}, {u'type': u'string', u'name': u'Flooring'}, {u'type': u'boolean', u'name': u'GarageYN'}, {u'type': u'string', u'name': u'Levels'}, {u'type': u'bigint', u'name': u'LivingArea'}, {u'type': u'boolean', u'name': u'PoolPrivateYN'}, {u'type': u'bigint', u'name': u'PostalCode'}, {u'type': u'string', u'name': u'PropertySubType'}, {u'type': u'string', u'name': u'Roof'}, {u'type': u'string', u'name': u'Sewer'}, {u'type': u'string', u'name': u'StateOrProvince'}, {u'type': u'string', u'name': u'StreetName'}, {u'downcastedToStringFromMeaning': u'LongMeaning', u'type': u'string', u'name': u'StreetNumber'}, {u'type': u'boolean', u'name': u'ViewYN'}, {u'type': u'string', u'name': u'WaterSource'}, {u'type': u'boolean', u'name': u'WaterfrontYN'}, {u'type': u'bigint', u'name': u'YearBuilt'}, {u'type': u'double', u'name': u'UrbanPercent'}, {u'type': u'double', u'name': u'MalePopPercentage'}, {u'type': u'double', u'name': u'MaxRacePopPercent'}, {u'type': u'double', u'name': u'MultiRacePopPercent'}, {u'type': u'double', u'name': u'OwnMortgagePercentage'}, {u'type': u'double', u'name': u'OwnNoMortgagePercentage'}, {u'type': u'double', u'name': u'RentersPercentage'}, {u'type': u'double', u'name': u'AvgHouseholdSize'}, {u'type': u'double', u'name': u'HouseholdsMoreThan3GensPercetage'}, {u'type': u'double', u'name': u'AdjMinGrossIncomePerReturn'}, {u'type': u'double', u'name': u'AdjListPrice'}]}\n",
    "\n",
    "ml_dataset_handle = dataiku.Dataset('casa_cluster_austin_only')\n",
    "ml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n",
    "%time ml_dataset = ml_dataset_handle.get_dataframe(limit = 100000)\n",
    "\n",
    "print ('Base data has %i rows and %i columns' % (ml_dataset.shape[0], ml_dataset.shape[1]))\n",
    "# Five first records\",\n",
    "ml_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing aims at making the dataset compatible with modeling.\n",
    "At the end of this step, we will have a matrix of float numbers, with no missing values.\n",
    "We'll use the features and the preprocessing steps defined in Models.\n",
    "\n",
    "Let's only keep selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dataset = ml_dataset[[u'PropertySubType', u'LivingArea', u'AvgHouseholdSize', u'GarageYN', u'WaterfrontYN', u'BedroomsTotal', u'UrbanPercent', u'Flooring', u'FireplacesTotal', u'Levels', u'AdjListPrice', u'OwnMortgagePercentage', u'MultiRacePopPercent', u'AdjMinGrossIncomePerReturn', u'BathroomsFull', u'Roof', u'ViewYN', u'RentersPercentage', u'Sewer', u'YearBuilt', u'MalePopPercentage', u'MaxRacePopPercent', u'OwnNoMortgagePercentage', u'HouseholdsMoreThan3GensPercetage', u'PoolPrivateYN', u'BathroomsHalf', u'WaterSource', u'CoveredSpaces']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first coerce categorical columns into unicode, numerical features into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# astype('unicode') does not work as expected\n",
    "\n",
    "def coerce_to_unicode(x):\n",
    "    if sys.version_info < (3, 0):\n",
    "        if isinstance(x, str):\n",
    "            return unicode(x,'utf-8')\n",
    "        else:\n",
    "            return unicode(x)\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "\n",
    "categorical_features = [u'PropertySubType', u'GarageYN', u'WaterfrontYN', u'Flooring', u'Levels', u'Roof', u'ViewYN', u'Sewer', u'PoolPrivateYN', u'WaterSource']\n",
    "numerical_features = [u'LivingArea', u'AvgHouseholdSize', u'BedroomsTotal', u'UrbanPercent', u'FireplacesTotal', u'AdjListPrice', u'OwnMortgagePercentage', u'MultiRacePopPercent', u'AdjMinGrossIncomePerReturn', u'BathroomsFull', u'RentersPercentage', u'YearBuilt', u'MalePopPercentage', u'MaxRacePopPercent', u'OwnNoMortgagePercentage', u'HouseholdsMoreThan3GensPercetage', u'BathroomsHalf', u'CoveredSpaces']\n",
    "text_features = []\n",
    "from dataiku.doctor.utils import datetime_to_epoch\n",
    "for feature in categorical_features:\n",
    "    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\n",
    "for feature in text_features:\n",
    "    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\n",
    "for feature in numerical_features:\n",
    "    if ml_dataset[feature].dtype == np.dtype('M8[ns]'):\n",
    "        ml_dataset[feature] = datetime_to_epoch(ml_dataset[feature])\n",
    "    else:\n",
    "        ml_dataset[feature] = ml_dataset[feature].astype('double')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy our dataset to keep it for eventual profiling at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset will be the one on which we will apply ml technics\n",
    "train = ml_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do at the features level is to handle the missing values.\n",
    "Let's reuse the settings defined in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows_when_missing = []\n",
    "impute_when_missing = [{'impute_with': u'MEAN', 'feature': u'LivingArea'}, {'impute_with': u'MEAN', 'feature': u'AvgHouseholdSize'}, {'impute_with': u'MEAN', 'feature': u'BedroomsTotal'}, {'impute_with': u'MEAN', 'feature': u'UrbanPercent'}, {'impute_with': u'MEAN', 'feature': u'FireplacesTotal'}, {'impute_with': u'MEAN', 'feature': u'AdjListPrice'}, {'impute_with': u'MEAN', 'feature': u'OwnMortgagePercentage'}, {'impute_with': u'MEAN', 'feature': u'MultiRacePopPercent'}, {'impute_with': u'MEAN', 'feature': u'AdjMinGrossIncomePerReturn'}, {'impute_with': u'MEAN', 'feature': u'BathroomsFull'}, {'impute_with': u'MEAN', 'feature': u'RentersPercentage'}, {'impute_with': u'MEAN', 'feature': u'YearBuilt'}, {'impute_with': u'MEAN', 'feature': u'MalePopPercentage'}, {'impute_with': u'MEAN', 'feature': u'MaxRacePopPercent'}, {'impute_with': u'MEAN', 'feature': u'OwnNoMortgagePercentage'}, {'impute_with': u'MEAN', 'feature': u'HouseholdsMoreThan3GensPercetage'}, {'impute_with': u'MEAN', 'feature': u'BathroomsHalf'}, {'impute_with': u'MEAN', 'feature': u'CoveredSpaces'}]\n",
    "\n",
    "# Features for which we drop rows with missing values\"\n",
    "for feature in drop_rows_when_missing:\n",
    "    train = train[train[feature].notnull()]\n",
    "    \n",
    "    print ('Dropped missing records in %s' % feature)\n",
    "\n",
    "# Features for which we impute missing values\"\n",
    "for feature in impute_when_missing:\n",
    "    if feature['impute_with'] == 'MEAN':\n",
    "        v = train[feature['feature']].mean()\n",
    "    elif feature['impute_with'] == 'MEDIAN':\n",
    "        v = train[feature['feature']].median()\n",
    "    elif feature['impute_with'] == 'CREATE_CATEGORY':\n",
    "        v = 'NULL_CATEGORY'\n",
    "    elif feature['impute_with'] == 'MODE':\n",
    "        v = train[feature['feature']].value_counts().index[0]\n",
    "    elif feature['impute_with'] == 'CONSTANT':\n",
    "        v = feature['value']\n",
    "    train[feature['feature']] = train[feature['feature']].fillna(v)\n",
    "    \n",
    "    print ('Imputed missing values in feature %s with value %s' % (feature['feature'], coerce_to_unicode(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now handle the categorical features (still using the settings defined in Models):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dummy-encode the following features.\n",
    "A binary column is created for each of the 100 most frequent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_DUMMIES = 100\n",
    "\n",
    "categorical_to_dummy_encode = [u'PropertySubType', u'GarageYN', u'WaterfrontYN', u'Flooring', u'Levels', u'Roof', u'ViewYN', u'Sewer', u'PoolPrivateYN', u'WaterSource']\n",
    "\n",
    "# Only keep the top 100 values\n",
    "def select_dummy_values(train, features):\n",
    "    dummy_values = {}\n",
    "    for feature in categorical_to_dummy_encode:\n",
    "        values = [\n",
    "            value\n",
    "            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n",
    "        ]\n",
    "        dummy_values[feature] = values\n",
    "    return dummy_values\n",
    "\n",
    "DUMMY_VALUES = select_dummy_values(train, categorical_to_dummy_encode)\n",
    "\n",
    "def dummy_encode_dataframe(df):\n",
    "    for (feature, dummy_values) in DUMMY_VALUES.items():\n",
    "        for dummy_value in dummy_values:\n",
    "            dummy_name = u'%s_value_%s' % (feature, coerce_to_unicode(dummy_value))\n",
    "            df[dummy_name] = (df[feature] == dummy_value).astype(float)\n",
    "        del df[feature]\n",
    "        print ('Dummy-encoded feature %s' % feature)\n",
    "\n",
    "dummy_encode_dataframe(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rescale numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_features = {u'BathroomsHalf': u'AVGSTD', u'OwnMortgagePercentage': u'AVGSTD', u'MaxRacePopPercent': u'AVGSTD', u'LivingArea': u'AVGSTD', u'AdjListPrice': u'AVGSTD', u'BathroomsFull': u'AVGSTD', u'OwnNoMortgagePercentage': u'AVGSTD', u'FireplacesTotal': u'AVGSTD', u'HouseholdsMoreThan3GensPercetage': u'AVGSTD', u'AvgHouseholdSize': u'AVGSTD', u'RentersPercentage': u'AVGSTD', u'AdjMinGrossIncomePerReturn': u'AVGSTD', u'YearBuilt': u'AVGSTD', u'MalePopPercentage': u'AVGSTD', u'CoveredSpaces': u'AVGSTD', u'UrbanPercent': u'AVGSTD', u'BedroomsTotal': u'AVGSTD', u'MultiRacePopPercent': u'AVGSTD'}\n",
    "for (feature_name, rescale_method) in rescale_features.items():\n",
    "    if rescale_method == 'MINMAX':\n",
    "        _min = train[feature_name].min()\n",
    "        _max = train[feature_name].max()\n",
    "        scale = _max - _min\n",
    "        shift = _min\n",
    "    else:\n",
    "        shift = train[feature_name].mean()\n",
    "        scale = train[feature_name].std()\n",
    "    if scale == 0.:\n",
    "        del train[feature_name]\n",
    "        \n",
    "        print ('Feature %s was dropped because it has no variance' % feature_name)\n",
    "    else:\n",
    "        print ('Rescaled %s' % feature_name)\n",
    "        train[feature_name] = (train[feature_name] - shift).astype(np.float64) / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from train set\n",
    "from dataiku.doctor.preprocessing.dataframe_preprocessing import detect_outliers\n",
    "\n",
    "outliers = detect_outliers(train, 0.9, 100, 0.01)\n",
    "train = train[~outliers]\n",
    "\n",
    "print (\"%s outliers found\" % (outliers.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clustering_model = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally cluster our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clusters = clustering_model.fit_predict(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build up our result dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (clustering_model.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette = silhouette_score(train.values, clusters, metric='euclidean', sample_size=2000)\n",
    "print (\"Silhouette score :\", silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join our original dataset with the cluster labels we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = train.join(pd.Series(clusters, index=train.index, name='cluster'))\n",
    "final['cluster'] = final['cluster'].map(lambda cluster_id: 'cluster' + str(cluster_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cluster sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = pd.DataFrame({'size': final['cluster'].value_counts()})\n",
    "size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a nice scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_x = train.columns[0]   # change me\n",
    "axis_y = train.columns[1]  # change me\n",
    "\n",
    "from ggplot import ggplot, aes, geom_point\n",
    "print(ggplot(aes(axis_x, axis_y, colour='cluster'), final) + geom_point())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. It's now up to you to tune your preprocessing, your algo, and your analysis !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  },
  "name": "Clustering casa_cluster_austin_only"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
